import argparse
import os
import pickle as pkl

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.inspection import permutation_importance
import logging
from tqdm import tqdm

# from plotting import ResultsPlotter


# Utilities
def load_data(infile, outdir, overwrite=False):
    """Loads from npz generated by `condense_training_data.py` and creates dict to easily parse from."""
    if not os.path.exists(os.path.join(outdir, "data_backup.pkl")) or overwrite == True:
        sv_data = np.load(infile)
        samps = {"data": [], "labs": [], "files": []}
        print(samps)
        for id in tqdm(sv_data.files):
            samps["data"].append(sv_data[id])
            samps["labs"].append(id.split("-")[-2])
            samps["files"].append(id)
        with open(os.path.join(outdir, "data_backup.pkl"), "wb") as backupfile:
            pkl.dump(samps, backupfile)
    else:
        with open(os.path.join(outdir, "data_backup.pkl"), "rb") as backupfile:
            samps = pkl.load(backupfile)

    return samps


def split_data(data, labs, files):
    """
    Partitions samples into balanced train/test sets and then loads files into memory.
    """
    print(f"{len(data) } samples found.\n")

    X_train, X_test, files_train, files_test, y_train, y_test = train_test_split(
        data,
        files,
        labs,
        test_size=0.05,
        random_state=42,
        stratify=labs,
    )

    return X_train, X_test, files_train, files_test, y_train, y_test


def dump_model(clf, filename="sv_rf_geno_all.pkl"):
    """Dumps model to disk in pickle format."""
    with open(filename, "wb") as pklfile:
        pkl.dump(clf, pklfile)


def train_rf(X_train, y_train, class_lab, outdir):
    """
    Fits a random forest classifier to data, dumps to disk.
    """
    clf = RandomForestClassifier(
        n_estimators=500,
        random_state=42,
        class_weight="balanced",
        n_jobs=8,
        verbose=1,
    )

    clf.fit(X_train, y_train)
    dump_model(clf, os.path.join(outdir, f"svrf_classifier_{class_lab}.pkl"))
    
    # Built-in feature importances
    #built_in_importances = clf.feature_importances_
    #
    ## Permutation feature importances
    #result = permutation_importance(clf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=-1)
    #perm_importances = result.importances_mean
    #perm_std = result.importances_std
#
    ## Combine both feature importances in a table
    #indices = np.argsort(built_in_importances)[::-1]  # Sorting by built-in importance as a baseline
#
    ## Save the feature importances to a text file
    #with open(os.path.join(outdir, f"feature_importances_{class_lab}.txt"), 'w') as f:
    #    f.write("Feature\tBuilt-in Importance\tPermutation Importance\tPermutation Std\n")
    #    for i in indices:
    #        f.write(
    #            f"{i}\t{built_in_importances[i]:.6f}\t{perm_importances[i]:.6f}\t{perm_std[i]:.6f}\n"
    #        )
#
    #print(f"Feature importances saved to {os.path.join(outdir, f'feature_importances_{class_lab}.txt')}")

    return clf


def train_rf_regressor(X_train, y_train, class_lab, outdir):
    """
    Fits a random forest classifier to data, dumps to disk.
    """
    clf = RandomForestRegressor(
        n_estimators=500,
        random_state=42,
        # class_weight="balanced",
        n_jobs=-1,
        verbose=1,
    )

    clf.fit(X_train, y_train)
    dump_model(clf, os.path.join(outdir, f"svrf_{class_lab}.pkl"))

    return clf


# Pairwise
def filter_dict(dict, class_lab):
    """Filters data dictionary to only controls + class label for pairwise training."""
    print(dict["labs"][1])
    filtered_dict = {key: [] for key in dict.keys()}
    for i in range(len(dict["labs"])):
        if dict["labs"][i] == "0" or dict["labs"][i] == str(class_lab):
            filtered_dict["data"].append(dict["data"][i])
            filtered_dict["labs"].append(dict["labs"][i])
            filtered_dict["files"].append(dict["files"][i])

    return filtered_dict


def pred_pairwise_rf(rf, X_test, y_test, files_test, data_class, outdir):
    """
    Uses trained RF to predict on test data.
    Writes results to tsv and returns data for plotting.
    """
    preds = rf.predict(X_test)
    probs = rf.predict_proba(X_test)
    print(preds)

    pd.DataFrame(
        {
            "file": files_test,
            "true": y_test,
            "pred": preds,
            "cntrl_score": probs[:, 0],
            f"{data_class}_score": probs[:, 1],
        }
    ).to_csv(os.path.join(outdir, f"test_predictions_{data_class}.csv"), index=False)

    return y_test, preds


def pairwise_runner(data_dict, slab, ilab, outdir):
    """
    Iteratively trains control/class RF models.
    """
    print(f"\n[info] Running with {slab.upper()}")
    print(f"[info] Integer class is {ilab}")
    outdir = outdir
    samps = filter_dict(data_dict, ilab)
    print("[info] Size of dataset", len(samps["labs"]))
    (
        X_train_arr,
        X_test_arr,
        files_train,
        files_test,
        y_train,
        y_test,
    ) = split_data(samps["data"], samps["labs"], samps["files"])

    rf = train_rf(X_train_arr, y_train, slab, outdir)
    y_test, preds = pred_pairwise_rf(rf, X_test_arr, y_test, files_test, slab, outdir)

    return y_test, preds


# Multiclass
def pred_all_rf(rf, X_test, y_test, files_test, data_class_list, outdir):
    """
    Uses trained RF to predict on multiclass test data.
    """
    preds = rf.predict(X_test)
    probs = rf.predict_proba(X_test)
    res_dict = {
        "file": files_test,
        "true": y_test,
        "pred": preds,
        "cntrl_score": probs[:, 0],
    }
    for i, c in enumerate(data_class_list):
        res_dict[f"{c}_score"] = probs[:, i]

    pd.DataFrame(res_dict).to_csv(os.path.join(outdir, f"test_predictions_all.csv"))

    return y_test, preds


def pred_all_rf_regressor(rf, X_test, y_test, files_test, data_class_list, outdir):
    """
    Uses trained regressor to predict on regression test data.
    """
    # Predictions for the test set
    preds = rf.predict(X_test)
    print(preds)
    # print(X_test)
    y_test = [float(value) for value in y_test]
    y_test = np.array(y_test)
    print(y_test)
    # print("y_test dtype:", y_test.dtype)
    # print("preds dtype:", preds.dtype)
    ## Calculate and print the mean squared error (or any other regression metric you prefer)
    mse = mean_squared_error(y_test, preds)
    print(f"Mean Squared Error on Test Set: {mse}")
    # Save the trained regressor
    # dump_model(regressor, os.path.join(outdir, "regressor_model.pkl"))
    # Save predictions to a CSV file
    res_dict = {
        "file": files_test,
        "true": y_test,
        "pred": preds,
        "cntrl_score": preds,  # Example, you can add more columns as needed
    }
    # Create a DataFrame and save it to a CSV file
    # pd.DataFrame(res_dict).to_csv(os.path.join(outdir, "test_predictions_all_regressor.csv"))

    return y_test, preds


def all_runner(samps, data_classes, outdir):
    """
    Trains multiclass RF model.
    """

    print("Size of samps", len(samps))
    (
        X_train_arr,
        X_test_arr,
        files_train,
        files_test,
        y_train,
        y_test,
    ) = split_data(samps["data"], samps["labs"], samps["files"])

    rf = train_rf(X_train_arr, y_train, "all", outdir)
    # rf = train_rf_regressor(X_train_arr, y_train, "all", outdir)
    y_test, preds = pred_all_rf(
        rf, X_test_arr, y_test, files_test, data_classes, outdir
    )
    # y_test, preds = pred_all_rf_regressor(
    #    rf, X_test_arr, y_test, files_test, data_classes, outdir
    # )
    return y_test, preds


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-i",
        "--input-npz",
        metavar="INFILE",
        required=True,
        dest="infile",
        help="NPZ file created using condense_training_data.",
    )
    parser.add_argument(
        "-o",
        "--outdir",
        metavar="OUTDIR",
        required=False,
        default=os.getcwd(),
        dest="outdir",
        help="Directory to write condensed data to. Defaults to cwd.",
    )

    return parser.parse_args()


def main():
    ap = parse_args()

    print("[info] Loading data")
    data_dict = load_data(ap.infile, ap.outdir)
    print("[info] Data loaded")
    class_labs = ["nonrefte"]
    # "del", "ins", "cnv", "inv",
    pairwise = False
    if pairwise:
        # Pairwise

        print("[info] Starting Control vs Single Class Training")
        # for ilab, slab in enumerate(class_labs, 1):
        #    y_test, preds = pairwise_runner(data_dict, slab, ilab, ap.outdir)
    #
    #    pltr = ResultsPlotter(
    #        y_test, preds, target_names=["cntrl", slab], working_dir=ap.outdir
    #    )
    #    pltr.plot_confusion_matrix(
    #        title=f"Genotype_RF_{slab.upper()}", normalize=True
    #    )
    #    pltr.print_classification_report()

    multi = True
    if multi:
        # Multiclass
        y_test, preds = all_runner(data_dict, class_labs, ap.outdir)
        # pltr = ResultsPlotter(
        #    y_test, preds, target_names=["cntrl"] + class_labs, working_dir=ap.outdir
        # )
        # pltr.plot_confusion_matrix(title=f"Genotype_RF_Multiclass", normalize=True)
        # pltr.print_classification_report()


if __name__ == "__main__":
    main()
